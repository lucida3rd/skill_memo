-----------------------
mastodon チューニングメモ

Lucida（lucida3rd@mstdn.lucida-memo.info）
-----------------------
＜概要＞
mastodonの最適化、調整技のメモ。

◆PgBouncerのインストール：CentOS版
◆mastodonのサービスチューニング
◆ImageMagicチューニング


---------------------------------
＜注意事項＞
---------------------------------
・斜め読みしないで、各メモの効果、手順をよく確認してください。
  よくわかんないうちは使用しないでください。
・各メモには元ソースのリンクを貼り付けてますので、
  よくわかんないときはそちらも合わせて確認してください。
・このメモを使用したことによる不具合、損害について当方は責任を持ちません。
  全て自己責任でお願いします。
・当メモの質問は受け付けません。自己対応でお願いします。



-----------------------
＜PgBouncerのインストール：CentOS版＞

mastodonに用意されているサポートソフトみたいなもん？
デフォルトだとpostgreのプロセスを多めに使うためメモリを大量に使うことになる。
PgBouncerを使うとプロセスの数を調整でき、メモリの消費を抑えることができるらしい。

なお、PgBouncerのマニュアルだと手順が大分異なるのと、追加設定が必要だったりする。
参考にしたのは以下のサイトだけどだいぶ違う。

参考：
http://fnya.cocolog-nifty.com/blog/2017/12/mastodon-centos.html

要するにOSごとのリポジトリ（postgreのドライバみたいなもん？）を先にインストールする
ことになってるらしい。

1.OSのバージョンを確認する。
　さくらインターネットの場合、VPSのサービス情報に記載されている。

2.postgreのバージョンを確認する。
# psql -V

3.postgreのページからリポジトリを選ぶ。
　1と2の結果で選択できる。
https://yum.postgresql.org/repopackages.php

4.あらかじめlibeventをインストールする。 ※てかいらないんじゃね？って思った
# yum install libevent-devel

5.リポジトリをダウンロードしてインストールする。
# wget [リポジトリのURL]
# rpm -ivh [リポジトリのファイル名]

6.PgBouncerをインストールする。
# yum -y install pgbouncer
# systemctl restart pgbouncer
# systemctl enable pgbouncer

7.bgBouncerにパスワードを設定する。まずiniにアクセスする。
# cd /etc/pgbouncer

8.パスワードのハッシュ値を取得する。
# echo -n "パスワード+mastodon" | md5sum
   +は省略する。mastodonはユーザ名。

9.カレントにuserlist.txtを作成し、中にmastodonと9のハッシュ値を入れる。
# vi userlist.txt
書式： "mastodon" "md5[10のハッシュ値]"

10. 8.9と同じ要領でpgbouncerユーザを追加する。（パスワードも別で決め設定すること）

12.pgbouncer.ini の設定をする。

[databases]
[db名] = host=127.0.0.1 port=5432 dbname=[db名] user=mastodon password=[パスワード]    ※追加

[pgbouncer]
auth_type = md5
admin_users = pgbouncer
pool_mode = transaction

※この２つはデフォルトで問題ないけど調整可能みたい
max_client_conn = 100
default_pool_size = 20


13.保存したらpostgresqlのログイン設定をmd5に変更する。
   ※ユーザ個別、DB個別でも設定できるっぽいけど、まあまあ。
# vi /var/lib/pgsql/9.6/data/pg_hba.conf
------
# TYPE  DATABASE        USER            ADDRESS                 METHOD
###local   all             all                                     peer
local   all             all                                     md5
# IPv4 local connections:
###host    all             all             127.0.0.1/32            ident
host    all             all             127.0.0.1/32            md5
------

14.pgbouncerとpostgresqlのサービスを再起動する。
# systemctl restart postgresql*
# systemctl restart pgbouncer
    ここで何も表示がでないことを確認する。

15.接続テスト。pgbouncerからmastodonのDBに接続できることを確認する。
# psql -p 6432 -h 127.0.0.1 -U mastodon [db名]
mastodon> 
  sqlのプロンプトになればOK。 \q  で抜ける。
  auth errorで蹴られたら設定の見直し。9項あたりが怪しいと思われます。

※もしかしたらここからmastodonが接続不良になるかもなので、手早くやる。


16.mastodonのコンフィグを変更する。パスワードの設定と、DBのポートを変更する設定。
# vim /home/mastodon/live/.env.production

PREPARED_STATEMENTS=false  ※一番上のほうに追加
DB_HOST=localhost
DB_USER=mastodon
DB_NAME=[db名]
DB_PASS=[password]  ※パスワードを設定
DB_PORT=6432        ※ここも変更する

17.mastodonサービスを再起動する。
# systemctl restart mastodon*

※mastodonのページで500、502が出なくなればOK。
　5分くらい不安定になることもあるかも。


-----------------------
＜postgreSQLのチューニング＞

参考：
https://qiita.com/cuzic/items/f9b846e6171a54079d77

pgTuneのパラメータ調整で最適値を設定できる。

# cd /var/lib/pgsql/[バージョン]/data
# vi postgresql.conf

https://pgtune.leopard.in.ua/#/

左欄に環境値を入力。[Genelate]クリックで右欄にチューニング項目が出る。

いじったら反映する
# systemctl restart postgresql*
# systemctl restart pgbouncer


各項目の詳細：

max_connections
RDBMSへの同時接続数。
WEBなどのユーザが同時にアクセスすることが想定される環境であれば大きく設定する。
バッチ処理など同時アクセス数が少ない環境では小さくする。
大きく設定しすぎるとメモリ消費量が増えてしまう。
　設定値：よくわかんないけど、お一人様でも蔵とか使うし、20くらい？

shared_buffers
データベースが使用する共有メモリバッファの値を指定する。
値が大きすぎるとOSがバッファとして使える容量が減り、逆に性能が低下する。
　設定値：総メモリの 25% 程度

effective_cache_size
カーネルや PostgeSQL の共有バッファなど PostgreSQL が使用するバッファ領域、
ディスクキャッシュとして利用可能な大きさの推定値。
この値を大きくするとオプティマイザがインデックスを利用した問合せプランを
選択しやすくなる。
　設定値：総メモリの 50% 程度

work_mem
ソートやテーブルのジョインなどのときに使われるメモリ量。
この値を大きくすると、テーブル結合処理などが高速化される。
　設定値：総メモリの 0.5% 程度？

maintenance_work_mem
vacuum 処理で使われるメモリ量。
　設定値：256MB or 512MB 程度。

checkpoint_completion_target
チェックポイントの書き出し処理を分散し、ゆっくり書き出すようにするときに使う。
　設定値：0.9

random_page_cost
デフォルト値は 4.0。
高IOPSなRAID構成の磁気ディスクを使う場合は2.0～3.0。
SSD の場合は 1.1 に設定することで高速化できる。

effective_io_concurrency
通常は１度に１ブロックずつ取り出す処理を行うが、efffective_io_concurrencyを設定すると、
データを先読みして複数ブロックを同時に取得することができる。
efffective_io_concurrencyの値はディスクドライブの数を指定すると良い。
たとえば RAID10（ミラーリング＋ストライピング）で構成している場合は 4 と指定する。

synchronous_commit
offとすると同期コミットが無効化され、高速化できる。
ただし、クラッシュ時の復元可能性は担保されるものの、データ損失のリスクがある。


いじらなくていい：

fsync
synchronous_commit=off とすれば高速化効果としては十分で、fsync=off とする必要はない。

wal_buffers
PostgreSQL 9.1 以降では自動調整されるため、デフォルト値のままで問題ない。


-----------------------
＜mastodonのサービスチューニング＞

上記bgBouncerを導入するのが前提になると思う。
また併せてjmallocの導入も行う。

参考：
https://qiita.com/kumasun/items/bf4997f181f893130041


1.jemallocをインストールする。（最新を自分でmakeする方法）

release
https://github.com/jemalloc/jemalloc/releases

インストール
https://github.com/jemalloc/jemalloc/blob/dev/INSTALL.md

# wget [ソースコードのURL]
# tar xvzf [アーカイブファイル名]
# cd [解凍されたフォルダ]
# bash autogen,sh
# make dist
# make

# make install
# ls -la /usr/local/lib/libjemalloc*
とにかくlibjemalloc*を探す。

2.mastodon webサービスの調整。
# vi /etc/systemd/system/mastodon-web.service
......
Environment="PORT=3000"
Environment="WEB_CONCURRENCY=[コア数]"  ※以下全て追加
Environment="MAX_THREADS=5"
Environment="LD_PRELOAD=[jmallocファイルまでの絶対パス]"★
......

コア数分bundleプロセスができる。処理できるようなら調整できると思う。

3.mastodon ストリームサービスの調整。
# vi /etc/systemd/system/mastodon-streaming.service
......
ExecStart=/usr/bin/npm run start
↓
ExecStart=/usr/bin/node streaming/index.js
......
※node.jsのバージョンが古いとエラーがでるかも？

4.mastodon sidekiqサービスの調整。
# vi /etc/systemd/system/mastodon-sidekiq.service
Environment="DB_POOL=5"
ExecStart=/home/mastodon/.rbenv/shims/bundle exec sidekiq -c 5 -q default -q mailers -q pull -q push
↓
Environment="DB_POOL=30"
Environment="LD_PRELOAD=[jmallocファイルまでの絶対パス]"★
ExecStart=/home/mastodon/.rbenv/shims/bundle exec sidekiq -c 30 -q default -q push -q pull -q mailers

5.mastodonサービスを再起動する。
# systemctl daemon-reload
# systemctl restart mastodon*
   ※デーモンを先に再起動しないと怒られる


◆更なるチューニング：sidekiqを複数プロセスで実行する。

参考：
https://inside.pixiv.blog/harukasan/1284
https://qiita.com/Ress/items/14cf460ca929a4213236

Mastodonには標準で4つのキューがあるが、さくらスクリプトでデフォルトだと1つのsidekiqプロセスで動かしている。
このため優先されるキューの処理が先行されると、後のキューの処理が遅延し、全体的なsidekiqの処理が遅延してしまう。
（場合によってはmastodonのサービスが落ちるような動きになる。目視観測だが...）

キューの種類は以下の通り。
　default：トゥートの反映など全般
　mail   ：メールを送信する
　push   ：ほかのMastodonインスタンスに更新を送信する
　pull   ：ほかのMastodonインスタンスから更新を取得する

このうちpush、pullのキューは他のMastodonインスタンスのAPIをリクエストするため、
ほかのMastodonインスタンスが応答できない状態に陥ったり、自インスタンスのpushが多くなると
defaultキューの処理も遅延させてしまう。

これを防ぐため、キューごとにSidekiqプロセスを冗長化する。

sidekiqは起動オプションでどのキューを処理するか指定することができるので、
これを利用して1台のサーバインスタンスに複数のSidekiqプロセスを立てることにする。

例に従い、以下のプロセスとDBプール配分にする。（masdn.lucida-memo.infoの例）
  プロセス1：default,mailers  DB Pool=20   ※これは既存のmastodon-sidekiq.serviceを使おうと思う
  プロセス2：push             DP Pool=20   ※mastodon-sidekiq-push
  プロセス3：pull             DB Pool=20   ※mastodon-sidekiq-pull

1.サービスファイルを作成する

vi /etc/systemd/system/mastodon-sidekiq.service
[Unit]
Description=mastodon-sidekiq
After=network.target

[Service]
Type=simple
User=mastodon
WorkingDirectory=/home/mastodon/live
Environment="RAILS_ENV=production"
Environment="DB_POOL=20"
Environment="LD_PRELOAD=[jmallocファイルまでの絶対パス]"★
ExecStart=/home/mastodon/.rbenv/shims/bundle exec sidekiq -c 20 -q default,2 -q mailers,1
TimeoutSec=15
Restart=always

[Install]
WantedBy=multi-user.target
----------------
vi /etc/systemd/system/mastodon-sidekiq-push.service
[Unit]
Description=mastodon-sidekiq-push
After=network.target

[Service]
Type=simple
User=mastodon
WorkingDirectory=/home/mastodon/live
Environment="RAILS_ENV=production"
Environment="DB_POOL=10"
Environment="LD_PRELOAD=[jmallocファイルまでの絶対パス]"★
ExecStart=/home/mastodon/.rbenv/shims/bundle exec sidekiq -c 10 -q push
TimeoutSec=15
Restart=always

[Install]
WantedBy=multi-user.target
----------------
vi /etc/systemd/system/mastodon-sidekiq-pull.service
[Unit]
Description=mastodon-sidekiq-pull
After=network.target

[Service]
Type=simple
User=mastodon
WorkingDirectory=/home/mastodon/live
Environment="RAILS_ENV=production"
Environment="DB_POOL=10"
Environment="LD_PRELOAD=[jmallocファイルまでの絶対パス]"★
ExecStart=/home/mastodon/.rbenv/shims/bundle exec sidekiq -c 10 -q pull
TimeoutSec=15
Restart=always

[Install]
WantedBy=multi-user.target
----------------


2.サービスとして起動、登録する
# systemctl daemon-reload
# systemctl restart mastodon-sidekiq.service

# systemctl start mastodon-sidekiq-push.service
# systemctl enable mastodon-sidekiq-push.service

# systemctl start mastodon-sidekiq-pull.service
# systemctl enable mastodon-sidekiq-pull.service

# systemctl status mastodon*


-----------------------
＜ImageMagicチューニング＞

ImageMagicの個人makeインストール方法（CentOS版）。
あとmastodonの場合、png形式を扱う機会が多いので（iPhoneユーザ向け）
pngのライブラリも別途makeインストールする必要がある。

参考
https://gist.github.com/crakaC/9f8f33aa8179234b19148e602bc980f4#file-install_imagemagick-md


1.先にpngライブラリを展開する。
ライブラリのページからアーカイブをダウンロードする。
http://www.libpng.org/pub/png/libpng.html

# wget [アーカイブのリンク]
# tar xvzf [アーカイブファイル名]
# cd [解凍されたフォルダ]

2.メイクする。
# ./configure --enable-shared
# make

3.インストールする。
# make install

4.jpeg用にlibjpeg-develも必要
# yum install libjpeg-devel

5.ImageMagicのイメージを展開する。
ライブラリのフォルダからrootに移ってから作業する。
# cd ..
# wget https://www.imagemagick.org/download/ImageMagick.tar.gz
# tar xvzf ImageMagick.tar.gz
# cd [解凍されたフォルダ]

6.おまじない？？？
実行するとコケるけど気にしない。
pngのライブラリパスを通すことをやってるらしい。
# export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig
# ./configure LDFLAGS='-L/usr/local/lib -R/usr/local/lib'

7.gifのキャッシュを変更する
# vi [ImageMagick]/MagickCore/quantize.c
CacheShift = 3     //固定する

8.オプションを変更する
--with-quantum-depth  ※画像のbit数指定。デフォルトで16bitだけど、普通は8bitでも十分らしい。
--disable-openmp      ※シングルコアの場合効果あり
--without-threads     ※シングルコアの場合効果あり
--disable-hdri        ※HDRI変換を無効にすることでリソースが抑えられる
--with-png            ※pngの有効化。付けなくてもいい気もする。

★シングルコア向け
# ./configure --disable-hdri --with-quantum-depth=8 --with-png --with-jpeg --disable-openmp --without-threads
# make

★マルチコア向け
# ./configure --disable-hdri --with-quantum-depth=8 --with-png --with-jpeg
# make

実行したあと最後に表示される有効オプション結果はコピペで控えておいたほうがいいかも。
このとき--with-pngがyesになってることを確認する。
noのままであれば5でがんばってパスを通すこと。（どうやるんぢゃ？）

9.インストールする。
# make install
# ldconfig /usr/local/lib  ※やらなくていい気もする

10.動作確認
# convert -version
  今日の日付で～Q8～であればOK。
  いちおmastodon権限でも試してみる。


その他：
何らかの不手際でリメイクするときはmakeファイルをクリアすること。
# make clean



-----------------------
＜nginxチューニング＞
2個の記事を参考に。
https://inside.pixiv.blog/harukasan/1284
https://qiita.com/toshihirock/items/f3aac142882c9c320b7f

worker_rlimit_nofile、worker_connectionsをチューニングする。

worker_rlimit_nofile
ディスクリプリンタの値の設定ができる。ディスクリプリンタとは...わからんです。
OSのディスクリプリンタ値は以下で調べられる。
# $ ulimit -n

worker_connections
nginxではworker_rlimit_nofileの値以上を設定することができない。

ので、worker_rlimit_nofile <= worker_connections となる。

設定ファイルは以下
# vi /etc/nginx/nginx.conf

以下のように変更。（※qiitaの例）
worker_rlimit_nofile 2048;

events {
###	worker_connections 1024;
	worker_connections 2048;
}

変更したらnginxを再起動する。
# nginx -s reload


-----------------------
＜OSチューニング＞

参考
http://www.denet.ad.jp/technology/2017/11/redis-centos7.html

1.THPをdisableにする
今の設定値
# cat /sys/kernel/mm/transparent_hugepage/enabled
[always] madvise never  ///有効になっている。

無効化のコマンド
# echo never > /sys/kernel/mm/transparent_hugepage/enabled

設定後の確認
# cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]  ///無効になった

このままだとサーバリブートで元に戻ってしまうので。
# vi /etc/rc.local
最終行に以下を追記
echo never > /sys/kernel/mm/transparent_hugepage/enabled

実行権限を付与
# chmod +x /etc/rc.d/rc.local

radisサービスの設定を変える
# vi /usr/lib/systemd/system/redis.service

[Unit]
Description=Redis persistent key-value database
After=network.target rc-local.service  //「rc-local.service」を追記

サーバをリブートして設定が反映されるか確認する
# reboot
...
# cat /sys/kernel/mm/transparent_hugepage/enabled
always madvise [never]

# systemctl status redis.service


2.OSパラメータの調整。
tcp-backlogの値をメモしておく（変更はしない）
# vi /etc/redis.conf

設定変更
# vi /etc/sysctl.conf
vm.overcommit_memory=1  ※追加
  OOM Killerのふるまい。
  0：空き容量がなければ実行中のプロセスを強制終了する（デフォルト）
  1：制限ギリギリまでメモリがあるように振る舞い、確保できなければ実行中のプロセスを強制終了する
  2：空き容量がない場合はエラーを発生させる
  ※radisでは1が推奨されている

net.core.somaxconn = 511
  OSレベルの接続キューの最大値。
  具体的には「アプリで設定した指定した接続キューの最大数 > net.core.somaxconn設定値」の場合、
  アプリ側での設定値が大きくても、net.core.somaxconn設定値がボトルネックになって接続不可になる。
  ※/etc/redis.confのtcp-backlogの値を設定する（さくらスクリプトの場合は511）

vm.swappiness = 10  ※追加
  スワップパラメータ。
  値は0から100まででデフォルトは60。大きいほどよくスワップする。
  0にするとRAMを使いきるまでスワップしない。
  100だとパフォーマンスに悪影響を及ぼすレベルでガンガンスワップする。


サーバをリブートして設定が反映されるか確認する
# reboot
...
# cat /proc/sys/vm/overcommit_memory
# cat /proc/sys/net/core/somaxconn
# cat /proc/sys/vm/swappiness


メモ：
# sysctl -p


